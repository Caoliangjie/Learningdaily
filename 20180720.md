## Python学习
函数也可以作为一个返回值，具体用法例如:
```
def lazy_sum(*args):
   def sum():
         ax=0
         for n in args
               ax=ax+n
          return ax
    return sum
```
返回的是函数，而调用sum才是求和结果。这里因为arg是引用的局部变量，当他返回了一个函数后，新的局部变量还被新函数引用。
匿名函数，lambda这里是一个关键字，不能定义变量为这个名字。用法类似于
```
def  f（x）
       return x*x
```
这里lambda x：x*x和上方的f是等价的，但是它本身仍是函数，只有返回f（n）才能得到函数结果。
装饰器，函数也可以被赋值给变量， functools是一个模块，函数很多。
```
import functools

def log(func):
    @functools.wraps(func)
    def wrapper(*args, **kw):
        print('call %s():' % func.__name__)
        return func(*args, **kw)
    return wrapper
```
这里要在前面加@（虽然不懂为啥，规定吧）然后就可以直接调用decorator。
偏函数，也是从functools里引出的功能，比如int（）把字符串转为整数，默认转换为10进制（base=10），更改默认的base值可以转化为不同的进制。
### 面向对象的编程
类和实例是最主要的概念。比如student类创造的是一个类，实例是根据类创建出的对象。类起到模板的作用，然后我们可以定义一个__init__方法，创建实例就把其他属性加进去。
```
class Student(object):

    def __init__(self, name, score):
        self.name = name
        self.score = score
```
数据封装，就是说有的的类中私有的，外部不能直接就调用。
```
class Student(object):

    def __init__(self, name, score):
        self.name = name
        self.score = score

    def print_score(self):
        print('%s: %s' % (self.name, self.score))
```
这样只能直接再对此建一个新方法，令外部也可以运用内部的方法。
这就是访问限制问题，建立如下的get方法就好弄。
```
class Student(object):
    def __init__(self, name, gender):
        self.__name = name
        self.__gender = gender

    def get_gender(self):
        return self.__gender

    def set_gender(self, gender):
        self.__gender = gender

```
上面这个把gender对外隐藏，用get和set两个代替，并检查参数有没有错误。
其中object是可以调用别的类的，然后object此时作为该类的父亲，就会有传递属性保证属性是一致的。（继承性）
继承的话对于子类好处是很大的，因为不用自己搭环境了，直接用父类的。可以用isinstance判断变量类型。比如：
```
isinstance（a，list）
True
```
但是如果用于判断定好的子类后，会统一返回父类的类型。
python是一种动态语言，所以子类可以自带自己独特的方法，不一定完全和父类一致。type（）可以用来判断对象类型。比如：
```
>>> type(abs)
<class 'builtin_function_or_method'>
>>> type(a)
<class '__main__.Animal'>
```
dir（）函数可以获得一个对象所有属性和方法（适用于一个类）
自己写的类，那么需要写个自己定义__len的方法。
## Deep learning
介绍了三种经典模型： LeNet-5，AlexNet，VGG（VGG看过几次），随着深度增加，训练误差是会减小的。
Residual Network随着神经网络层数增加，training error会不断变小。当然，如果Residual blocks确实能训练得到非线性关系，那么也会忽略short cut，跟Plain Network起到同样的效果。

有一点需要注意的是，如果Residual blocks中a[l]
和a[l+2]的维度不同，通常可以引入矩阵Ws，与a[l]相乘，使得Ws∗a[l]的维度与a[l+2]一致。参数矩阵Ws有来两种方法得到：一种是将Ws作为学习参数，通过模型训练得到；另一种是固定Ws值（类似单位矩阵），不需要训练，Ws与a[l]的乘积仅仅使得a[l]截断或者补零。这两种方法都可行。
1*1 convolutions，特点是滤波器算子维度为1*1，对此卷积就是乘积。